{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0zkCz0/fvz1nebwdXrl3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamagami/is2024/blob/main/13_TransformerScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9GNJpqfnlIp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer scratch\n",
        "This example demonstrates the process of building and training a Transformer from scratch. It uses randomly generated feature data (sequence length 10, feature dimension 20) to perform a binary classification task (0 or 1). While the task itself is nonsensical, so the data is randomly generated, there is no inherent relationship between the labels and features, limiting the accuracy to 1/2. it provides a framework to understand the basic workings of a Transformer.\n",
        "\n",
        "For practical tasks, large datasets and significant computational resources are required. Therefore, the common approach is to use pre-trained models and fine-tune them for specific tasks."
      ],
      "metadata": {
        "id": "kY95cVXgnnSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "lWdJVT2gofqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Dataset class\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input': torch.tensor(self.data[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "uxgf9lTHohdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer model from scratch\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, num_classes, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)  # Pooling across sequence dimension\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "2dMCFUasonVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "data = np.random.rand(1000, 10, 20)  # 1000 samples, 10 sequence length, 20 input features\n",
        "labels = np.random.randint(0, 2, 1000)  # Binary classification (0 or 1)\n"
      ],
      "metadata": {
        "id": "J-Y9DKWoorPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_dataset = SimpleDataset(data[:800], labels[:800])\n",
        "val_dataset = SimpleDataset(data[800:], labels[800:])\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "q8OsTVGBoslz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model initialization\n",
        "input_dim = 20\n",
        "hidden_dim = 64\n",
        "num_heads = 4\n",
        "num_classes = 2\n",
        "num_layers = 2\n",
        "model = TransformerClassifier(input_dim, num_heads, num_classes, hidden_dim, num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMA8MjtFo0hM",
        "outputId": "e08956e9-2c14-4cfc-8d40-51149d50b2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs = batch['input']\n",
        "            labels = batch['label']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs = batch['input']\n",
        "                labels = batch['label']\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader)}, \"\n",
        "              f\"Val Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yWbsTlU8nly7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upx6okTVo4sd",
        "outputId": "0e12e196-4916-40d9-cffd-8d135f2930b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.7455205845832825, Val Loss: 0.6952246106587924, Accuracy: 47.5%\n",
            "Epoch 2/10, Train Loss: 0.6982923901081085, Val Loss: 0.7039937101877652, Accuracy: 47.5%\n",
            "Epoch 3/10, Train Loss: 0.7003650784492492, Val Loss: 0.692172110080719, Accuracy: 52.5%\n",
            "Epoch 4/10, Train Loss: 0.6998951327800751, Val Loss: 0.6941338043946487, Accuracy: 47.5%\n",
            "Epoch 5/10, Train Loss: 0.6969927203655243, Val Loss: 0.6931836971869836, Accuracy: 52.5%\n",
            "Epoch 6/10, Train Loss: 0.6992536985874176, Val Loss: 0.7048590504206144, Accuracy: 47.5%\n",
            "Epoch 7/10, Train Loss: 0.6948084306716918, Val Loss: 0.6974020004272461, Accuracy: 47.5%\n",
            "Epoch 8/10, Train Loss: 0.6957521033287049, Val Loss: 0.7029325320170476, Accuracy: 47.5%\n",
            "Epoch 9/10, Train Loss: 0.6953581845760346, Val Loss: 0.7169061440687913, Accuracy: 47.5%\n",
            "Epoch 10/10, Train Loss: 0.6912687551975251, Val Loss: 0.7218401844684894, Accuracy: 47.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6jkz2YcpBe_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}